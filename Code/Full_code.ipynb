{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"SpIzv_7a9x4j"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\jmanu\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\jmanu\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\jmanu\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\jmanu\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: contractions in c:\\users\\jmanu\\anaconda3\\lib\\site-packages (0.1.73)\n","Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\jmanu\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in c:\\users\\jmanu\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n","Requirement already satisfied: pyahocorasick in c:\\users\\jmanu\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"]}],"source":["\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","import pickle\n","\n","import re\n","import nltk\n","from nltk import pos_tag\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","!pip install contractions\n","from contractions import contractions_dict\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import Counter, defaultdict\n","\n","import torch\n","from torch.utils.data import Dataset, Subset, DataLoader\n","import torch.nn.functional as F\n","\n","from gensim.models import Word2Vec, KeyedVectors\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nn2SIBOH9x4s"},"source":["# Text Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2asivNvo9x4v"},"outputs":[],"source":["# Sample text to test on the main functions, each sentence is a special case\n","sample_texts = [\"Capital LeTTers arounD\",\n","                \"contraction's didn't you're won't can't\",\n","                \"carch3 @ @keep num#ber keep! did%nt (keep)  ^^^^keep u.s.\",\n","                \"123 adios23 d2do5 3not\",\n","                \"you them a are not\"]"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5KqEv4dL9x4w"},"outputs":[],"source":["# Convert to lowercase\n","def lowercase_text(raw_text):\n","    lowercase_text = [text.lower() for text in raw_text]\n","    return lowercase_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"515uPDSf9x4y","outputId":"691edd6e-3d73-4821-ebd1-7ee40039b3c6"},"outputs":[{"ename":"NameError","evalue":"name 'sample_texts' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mg:\\My Drive\\UNISA - Machine Learning\\Project\\Project.ipynb Cell 5\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/UNISA%20-%20Machine%20Learning/Project/Project.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test lower case\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/UNISA%20-%20Machine%20Learning/Project/Project.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lowercase_text(sample_texts)\n","\u001b[1;31mNameError\u001b[0m: name 'sample_texts' is not defined"]}],"source":["# Test lower case\n","lowercase_text(sample_texts)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IpkcazGW9x40"},"outputs":[],"source":["# Expand contractions\n","def expand_contractions(text):\n","    cleaned_text = []\n","    for sentence in text:\n","        expanded_text = [contractions_dict.get(word, word) for word in sentence.split()]\n","        cleaned_text.append(' '.join(expanded_text))\n","    return cleaned_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yg6o6O29x42","outputId":"24a1abdb-6835-4c4a-b681-9f8f7c6e3226"},"outputs":[{"data":{"text/plain":["['capital letters around',\n"," \"contraction's did not you are will not cannot\",\n"," 'carch3 @ @keep num#ber keep! did%nt (keep) ^^^^keep',\n"," '123 adios23 d2do5 3not',\n"," 'you them a are not']"]},"execution_count":299,"metadata":{},"output_type":"execute_result"}],"source":["# Test contraction expand\n","expand_contractions(sample_texts)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"o7afWrjv9x43"},"outputs":[],"source":["# Remove characters from the text and words with characters in them\n","# It keeps words with characters at the beggining or at the end\n","def remove_characters(raw_text):\n","    cleaned_text = []\n","    for text in raw_text:\n","        new_words = []\n","        words = text.split()\n","        for word in words:\n","            # Check if special characters are present in the word\n","            if re.search(r'[^a-zA-Z0-9]', word):\n","                # Check if they are at the start or end\n","                if word[0].isalnum() and word[-1].isalnum():\n","                    continue  # Special character inside the word\n","                else:\n","                    # Remove special characters from start and end\n","                    word = re.sub(r'^[^a-zA-Z0-9]+|[^a-zA-Z0-9]+$', '', word)\n","            new_words.append(word)\n","        cleaned_text.append(' '.join(new_words))\n","    return cleaned_text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbpacuBk9x45","outputId":"bae4c763-af0e-4ece-eea9-65d694c8a735"},"outputs":[{"data":{"text/plain":["['Capital LeTTers arounD',\n"," '',\n"," 'carch3  keep keep keep keep u.s',\n"," '123 adios23 d2do5 3not',\n"," 'you them a are not']"]},"execution_count":467,"metadata":{},"output_type":"execute_result"}],"source":["# Test the function\n","remove_characters(sample_texts)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ovajLtwX9x46"},"outputs":[],"source":["# Remove numbers or words with numbers\n","def remove_numbers(raw_text):\n","    cleaned_text =  [' '.join(re.sub(r'\\b\\w*\\d\\w*\\b', '', text).split()) for text in raw_text]\n","    return cleaned_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zEZ9MxVP9x47","outputId":"b74e1182-21e8-4046-b297-6bbdc263eaf4"},"outputs":[{"data":{"text/plain":["['Capital LeTTers arounD',\n"," \"contraction's didn't you're won't can't\",\n"," '@ @keep num#ber keep! did%nt (keep) ^^^^keep u.s.',\n"," '',\n"," 'you them a are not']"]},"execution_count":471,"metadata":{},"output_type":"execute_result"}],"source":["# Test the function\n","remove_numbers(sample_texts)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"XiQk64rC9x47"},"outputs":[],"source":["# Remove Stop Words\n","def remove_stopwords(raw_text):\n","    stop_words = set(stopwords.words('english'))\n","    cleaned_text = []\n","\n","    for sentence in raw_text:\n","        expanded_text = [word for word in sentence.split() if word not in stop_words]\n","        cleaned_text.append(' '.join(expanded_text))\n","    return cleaned_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udF-dEzj9x48","outputId":"389439ce-f808-4479-fccf-c66767d4d656"},"outputs":[{"data":{"text/plain":["['Capital LeTTers arounD',\n"," \"contraction's can't\",\n"," 'carch3 @ @keep num#ber keep! did%nt (keep) ^^^^keep u.s.',\n"," '123 adios23 d2do5 3not',\n"," '']"]},"execution_count":473,"metadata":{},"output_type":"execute_result"}],"source":["# Sample remove stopwords\n","remove_stopwords(sample_texts)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"L4ZQdU7H9x48"},"outputs":[],"source":["# Now we are going to do lemmantization, before this, we want to understand the word context on sentence\n","# We want to know if the word represents: adjetive, adverbe, verb, noun\n","\n","# Convert the part-of-speech naming scheme from the treebank to the wordnet format\n","def get_wordnet_pos(treebank_tag):\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN\n","\n","# Lemantaze with a word tag: adjetive, adverbe, verb, noun\n","def lemmatize_sentence(tagged_tokens):\n","    lemmatizer = WordNetLemmatizer()\n","    finish_text = []\n","\n","    for sentence in tagged_tokens:\n","        lemmatized_sentence = []\n","\n","        for word, tag in sentence:\n","            wntag = get_wordnet_pos(tag)\n","            lemmatized_sentence.append(lemmatizer.lemmatize(word, pos=wntag))\n","\n","        finish_text.append(lemmatized_sentence)\n","    return finish_text"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"vj4LtbEE9x49"},"outputs":[],"source":["# Build a function that will do each process\n","# The function will return the process text, with tokenized words on each sentence\n","def clean_text(text):\n","    lowr_text = lowercase_text(text)\n","    contraction_text = expand_contractions(lowr_text)\n","    char_text = remove_characters(contraction_text)\n","    number_text = remove_numbers(char_text)\n","    stopw_text = remove_stopwords(number_text)\n","    tag_tokenized_text = [pos_tag(word_tokenize(text)) for text in stopw_text]\n","    final_text = lemmatize_sentence(tag_tokenized_text)\n","    return final_text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EbDy8-o9x49","outputId":"42c1e4ab-6c78-4f49-84cd-b6622e924ff8"},"outputs":[{"data":{"text/plain":["[['capital', 'letter', 'around'],\n"," ['can', 'not'],\n"," ['keep', 'keep', 'keep', 'keep', 'u.s'],\n"," [],\n"," []]"]},"execution_count":476,"metadata":{},"output_type":"execute_result"}],"source":["# Test on sample text\n","clean_text(sample_texts)"]},{"cell_type":"markdown","metadata":{"id":"u-1XlffR9x4-"},"source":["# Prepare Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29197,"status":"ok","timestamp":1700084759653,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"-4gmKIjR-fDS","outputId":"c5da78c7-71de-4eac-93e8-c590674521dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1038,"status":"ok","timestamp":1700084783816,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"lSP-K9A7-nqh","outputId":"80c594bd-cd3a-45b0-c57b-a98b57055b4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","/content/drive/MyDrive/UNISA - Machine Learning/Project\n"]}],"source":["# Connect to the folder where I have the code\n","import os\n","!pwd\n","os.chdir('/content/drive/MyDrive/UNISA - Machine Learning/Project')\n","!pwd"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"84f7wZ1I9x4_"},"outputs":[],"source":["# Import the data I will use\n","df_train = pd.read_csv('./Genre Classification Dataset/train_data.txt',sep=' ::: ',header=None,engine='python', names=['Title','Genre','Description'])\n","df_test = pd.read_csv('./Genre Classification Dataset/test_data_solution.txt',sep=' ::: ',header=None,engine='python', names=['Title','Genre','Description'])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":611,"status":"ok","timestamp":1700084796708,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"i4YkuxX-9x4_","outputId":"2d34736e-6481-4ef7-aebd-738ac2e6ab73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: (54214, 3)\n","Test: (54200, 3)\n"]}],"source":["# look at the sapes\n","print(\"Train:\", df_train.shape)\n","print(\"Test:\", df_test.shape)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"_zxbS_Ee9x4_"},"outputs":[],"source":["# I'm going to merge them and do the splits myself\n","df = pd.concat([df_train, df_test])"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1700084803133,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"Tc3tk-y39x4_","outputId":"b4c807ec-6652-498e-efe7-d365e4c24e33"},"outputs":[{"data":{"text/plain":["Index(['drama', 'documentary', 'comedy', 'short', 'horror', 'thriller',\n","       'action', 'western', 'reality-tv', 'family'],\n","      dtype='object')"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Get the top 10 most used movie genres\n","df['Genre'].value_counts()[0:10].index"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700084807218,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"303nHoaW9x5A","outputId":"d41ec9f7-2556-4a46-ed8d-76a8c16feac0"},"outputs":[{"data":{"text/plain":["drama          27225\n","documentary    26192\n","comedy         14893\n","short          10145\n","horror          4408\n","thriller        3181\n","action          2629\n","western         2064\n","reality-tv      1767\n","family          1567\n","adventure       1550\n","music           1462\n","romance         1344\n","sci-fi          1293\n","adult           1180\n","crime           1010\n","animation        996\n","sport            863\n","talk-show        782\n","fantasy          645\n","mystery          637\n","musical          553\n","biography        529\n","history          486\n","game-show        387\n","news             362\n","war              264\n","Name: Genre, dtype: int64"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Is a really unbalanced dataset, I will work with the top 10 classes and make an even split, I will take off short and keep adventure instead.\n","df['Genre'].value_counts()"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"y4utwevu9x5A"},"outputs":[],"source":["# Create list for the genres I will use to clasiffy\n","genres = ['drama', 'documentary', 'comedy', 'adventure', 'horror', 'thriller',\n","       'action', 'western', 'reality-tv', 'family']"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"dUeeZM3j9x5A"},"outputs":[],"source":["# Short the dataframe for the top 10 most used genres\n","df = df[df[\"Genre\"].isin(genres)]\n","df = df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"l0vL5dFw9x5B"},"outputs":[],"source":["# Now I will balance the dataset to 1550 samples\n","# Create a new df\n","df_balanced= pd.DataFrame()\n","\n","# Loop through each genre and sample 1550 rows\n","for genre in df['Genre'].unique():\n","    genre_subset = df[df['Genre'] == genre]\n","    samples = genre_subset.sample(n=min(len(genre_subset), 1550), random_state=42)\n","    df_balanced = pd.concat([df_balanced, samples])\n","\n","# Reset the index of the new DataFrame\n","df_balanced = df_balanced.reset_index(drop=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":575,"status":"ok","timestamp":1700084821141,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"NeB6fzP-9x5B","outputId":"9f50c43f-e33b-4f7c-da3d-4878aaa270b8"},"outputs":[{"data":{"text/plain":["drama          1550\n","thriller       1550\n","documentary    1550\n","comedy         1550\n","reality-tv     1550\n","horror         1550\n","action         1550\n","adventure      1550\n","western        1550\n","family         1550\n","Name: Genre, dtype: int64"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Check if it worked\n","df_balanced[\"Genre\"].value_counts()"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"wOLInEdR9x5C"},"outputs":[],"source":["# Get the descriptions\n","corpus = df_balanced[[\"Description\"]].values"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"KBVU9soN9x5C"},"outputs":[],"source":["# Clean all descriptions\n","process_corpus = []\n","for sample in corpus:\n","    process_corpus.append(clean_text(sample)[0])"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700084978064,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"Oriu40uU9x5D","outputId":"46888424-96c7-4f44-d026-dbf74af61998"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original: An American Squad of GI's capture one Chinese POW and while they try to get back to their command post, they are killed one-by-one until only the POW is left. During this ordeal, the POW becomes close to his captors to the point he is heart-broken when they are killed and he is left to return home to his wife and baby. Length: 62\n","Process: ['american', 'squad', 'capture', 'one', 'chinese', 'pow', 'try', 'get', 'back', 'command', 'post', 'kill', 'pow', 'leave', 'ordeal', 'pow', 'become', 'close', 'captor', 'point', 'kill', 'leave', 'return', 'home', 'wife', 'baby'] Length: 26\n"]}],"source":["# Have a look at a sample\n","print(\"Original:\", df_balanced.iloc[0,2],\"Length:\",  len(df_balanced.iloc[0,2].split()))\n","print(\"Process:\", process_corpus[0], \"Length:\", len(process_corpus[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":496,"status":"ok","timestamp":1700085002819,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"dy7ilhvt9x5D","outputId":"67b3c2c4-a866-4300-cf5b-5d4ebe535eee"},"outputs":[{"data":{"text/plain":["{'drama': 58.0,\n"," 'documentary': 58.0,\n"," 'comedy': 52.0,\n"," 'adventure': 50.0,\n"," 'horror': 57.0,\n"," 'thriller': 53.0,\n"," 'action': 53.0,\n"," 'western': 66.0,\n"," 'reality-tv': 49.0,\n"," 'family': 53.0}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# Look at average description word length per genre\n","movies = {}\n","for genre in genres:\n","    total = 0\n","    for i in df_balanced[df_balanced[\"Genre\"] == genre].index:\n","        total += len(process_corpus[i])\n","    movies[genre] = round(total/1550, 0)\n","\n","# Print\n","movies"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":505,"status":"ok","timestamp":1700085015672,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"UpLt1rpb9x5E","outputId":"3a77e7af-4cbd-4ce8-c3cb-b98968c6a0f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 775\n","Val: 465\n","Test: 310\n"]}],"source":["# Now we can do the splits for the data\n","# Number of classes\n","num_classes = 10\n","\n","# Number of samples per class\n","samples_per_class = 1550\n","\n","# Splits for train, validate, and test\n","train_split = int(0.5 * samples_per_class)\n","val_split = int(0.3 * samples_per_class)\n","test_split = 1550 - train_split - val_split\n","\n","# Total Samples per Class\n","print(\"Train:\", train_split)\n","print(\"Val:\", val_split)\n","print(\"Test:\", test_split)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495,"status":"ok","timestamp":1700085021345,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"NHF-c4Ks9x5E","outputId":"5a1f07a1-0b3a-4bc7-ece0-1c84e10422f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 7750\n","Val: 4650\n","Test: 3100\n"]}],"source":["# Create lists to hold indices for each set\n","train_indices, val_indices, test_indices = [], [], []\n","\n","# Set random state\n","rng = np.random.default_rng(seed=42)\n","\n","# Split indices for each class\n","for class_index in range(num_classes):\n","    start_index = class_index * samples_per_class\n","    indices = list(range(start_index, start_index + samples_per_class))\n","    rng.shuffle(indices)\n","    train_indices.extend(indices[:train_split])\n","    val_indices.extend(indices[train_split:train_split + val_split])\n","    test_indices.extend(indices[train_split + val_split:])\n","\n","# Final lengths per class\n","print(\"Train:\", len(train_indices))\n","print(\"Val:\", len(val_indices))\n","print(\"Test:\", len(test_indices))"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"_n5nQzMp9x5F"},"outputs":[],"source":["# Create a full vocabulary\n","flatten_tokens = [word for sentence in process_corpus for word in sentence]\n","flatten_tokens = flatten_tokens + genres\n","word_counts = Counter(flatten_tokens)\n","vocab = {word: idx for idx, (word, _) in enumerate(word_counts.items())}\n","idx_to_word = {idx: word for word, idx in vocab.items()}"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700085332873,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"9UMydQWr-1kU","outputId":"354dbb83-f51a-4336-f72e-f1abbd65f7e7"},"outputs":[{"data":{"text/plain":["49732"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab)"]},{"cell_type":"markdown","metadata":{"id":"_94cfPct_gTe"},"source":["Courpus with less words - *not used*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1074,"status":"ok","timestamp":1700085124613,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"kb8SyxQf9x5G","outputId":"c58be7eb-091f-45d0-b2cd-959a995e4599"},"outputs":[{"data":{"text/plain":["35972"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["# remove non frequent words\n","remove_words = [word for word, count in word_counts.items() if count < 5]\n","len(remove_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiGXE2CH9x5G"},"outputs":[],"source":["# Placeholder for unknown words\n","placeholder = \"<UNKNOWN>\"\n","processed_corpus = []\n","\n","# Update corups\n","for sentence in process_corpus:\n","    new_sentence = [word if word not in remove_words else placeholder for word in sentence]\n","    processed_corpus.append(new_sentence)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCe0N6u4EWHe"},"outputs":[],"source":["with open('./processed_corpus.pkl', 'wb') as f:\n","    pickle.dump(processed_corpus, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKGHIfn7Ef5E"},"outputs":[],"source":["with open('my_list.pkl', 'rb') as f:\n","    processed_corpus = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQeNaD8G9x5H"},"outputs":[],"source":["# Update vocabulary\n","flatten_tokens = [word for sentence in processed_corpus for word in sentence]\n","flatten_tokens = flatten_tokens + genres\n","word_counts = Counter(flatten_tokens)\n","vocab = {word: idx for idx, (word, _) in enumerate(word_counts.items())}\n","idx_to_word = {idx: word for word, idx in vocab.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700036499118,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"fERn5R_79x5H","outputId":"70b00915-4b9e-41a3-b739-823a26c417cf"},"outputs":[{"data":{"text/plain":["13762"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["# Total words in new vocabulary\n","len(vocab)"]},{"cell_type":"markdown","metadata":{"id":"nP_Hub_S_llP"},"source":["Training Corpus - *Currently full corpus for traing the word2vec*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDkt7m2K-LLT"},"outputs":[],"source":["# Creating a training corpus with the training indices\n","train_corpus = [process_corpus[index] for index in train_indices]"]},{"cell_type":"markdown","metadata":{"id":"elga9mZA9x5S"},"source":["# Word2vec"]},{"cell_type":"markdown","metadata":{"id":"Lw0GF-aA9x5S"},"source":["1.- Gensim - Word2vec"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"EJHXbcbj9x5T"},"outputs":[],"source":["# Create the Word2Vec model\n","word2vec_g = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34531,"status":"ok","timestamp":1700085631011,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"-PZmr5HJ9x5T","outputId":"f521f44e-d398-469b-da80-5e959a7eeba1"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\jmanu\\OneDrive - University of South Australia\\Machine Learning\\Assessment\\Project\\Project.ipynb Cell 49\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model on our corpus\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Model trains fast, no need to send to device\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m word2vec_g\u001b[39m.\u001b[39mbuild_vocab(process_corpus)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m word2vec_g\u001b[39m.\u001b[39;49mtrain(process_corpus, total_examples\u001b[39m=\u001b[39;49mword2vec_g\u001b[39m.\u001b[39;49mcorpus_count, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1069\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     callback\u001b[39m.\u001b[39mon_epoch_begin(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1069\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_epoch(\n\u001b[0;32m   1070\u001b[0m         corpus_iterable, cur_epoch\u001b[39m=\u001b[39mcur_epoch, total_examples\u001b[39m=\u001b[39mtotal_examples,\n\u001b[0;32m   1071\u001b[0m         total_words\u001b[39m=\u001b[39mtotal_words, queue_factor\u001b[39m=\u001b[39mqueue_factor, report_delay\u001b[39m=\u001b[39mreport_delay,\n\u001b[0;32m   1072\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1074\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1075\u001b[0m         corpus_file, cur_epoch\u001b[39m=\u001b[39mcur_epoch, total_examples\u001b[39m=\u001b[39mtotal_examples, total_words\u001b[39m=\u001b[39mtotal_words,\n\u001b[0;32m   1076\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1430\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     thread\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m-> 1430\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_epoch_progress(\n\u001b[0;32m   1431\u001b[0m     progress_queue, job_queue, cur_epoch\u001b[39m=\u001b[39;49mcur_epoch, total_examples\u001b[39m=\u001b[39;49mtotal_examples,\n\u001b[0;32m   1432\u001b[0m     total_words\u001b[39m=\u001b[39;49mtotal_words, report_delay\u001b[39m=\u001b[39;49mreport_delay, is_corpus_file_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1433\u001b[0m )\n\u001b[0;32m   1435\u001b[0m \u001b[39mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1285\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1282\u001b[0m unfinished_worker_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\n\u001b[0;32m   1284\u001b[0m \u001b[39mwhile\u001b[39;00m unfinished_worker_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1285\u001b[0m     report \u001b[39m=\u001b[39m progress_queue\u001b[39m.\u001b[39;49mget()  \u001b[39m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m     \u001b[39mif\u001b[39;00m report \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m         unfinished_worker_count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[0;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train the model on our corpus\n","# Model trains fast, no need to send to device\n","word2vec_g.build_vocab(process_corpus)\n","word2vec_g.train(process_corpus, total_examples=word2vec_g.corpus_count, epochs=20)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":520,"status":"ok","timestamp":1700085659175,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"hTPKxwrs9x5U","outputId":"87f703d9-3bc9-4ef4-a430-6b6fc99a12af"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('african', 0.583674430847168), ('floridian', 0.5626319050788879), ('korean', 0.5395526885986328), ('expat', 0.5301021933555603), ('british', 0.5290839076042175), ('giannini', 0.5182136297225952), ('arab', 0.517073392868042), ('america', 0.5146300196647644), ('aboriginal', 0.5141898989677429), ('asian', 0.5075013637542725)]\n"]}],"source":["# Find words similar to 'american'\n","similar_words = word2vec_g.wv.most_similar('american')\n","print(similar_words)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":489,"status":"ok","timestamp":1700085673241,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"87YWewNR9x5U","outputId":"9396fb71-9835-4986-bff3-22144bce95e1"},"outputs":[{"data":{"text/plain":["array([ 0.6352159 ,  0.22406244, -1.5482981 , -1.0203586 ,  1.0783644 ,\n","        0.38738745,  1.308556  , -2.4453192 , -1.4578745 , -1.1502886 ,\n","       -0.17273381, -2.8118844 ,  1.3380165 , -3.726514  , -1.4046497 ,\n","       -2.1051283 , -1.6193353 ,  0.29824474,  0.37169066, -0.46930778,\n","       -0.9469707 ,  1.7773088 ,  0.05977992, -1.1591473 , -1.5467453 ,\n","        1.2519972 , -0.9482009 , -0.64813644, -2.8115933 , -0.5376743 ,\n","        1.3203669 ,  0.45369166, -0.96778727, -0.21026962,  0.5115575 ,\n","       -0.3134072 , -0.8402765 , -0.36334297,  0.8413578 , -1.6149932 ,\n","        0.25404006, -0.29458848,  0.7217553 ,  2.1788254 ,  0.19984618,\n","        0.61873984,  0.75563824,  0.05571314, -0.43087366, -2.1158206 ,\n","       -0.07347316, -1.5492835 ,  1.30679   , -3.2067845 , -2.6090105 ,\n","        2.7399142 ,  0.297596  ,  0.5840321 , -3.9002209 , -0.503715  ,\n","       -0.6403259 , -0.5064867 ,  0.19869536, -1.7935792 , -0.14431955,\n","        0.9113393 , -0.29766744, -0.5748825 , -0.49926427,  0.9586643 ,\n","       -1.837094  ,  1.6041093 , -0.74458754, -0.54814446,  1.6362977 ,\n","        1.0768738 ,  0.15062281,  0.18962657, -0.9515516 ,  1.3622242 ,\n","        0.71261173, -1.3325591 , -2.2978327 ,  1.2304205 ,  0.1559624 ,\n","        1.7014772 ,  0.21563224, -2.3191874 ,  0.03275706,  2.1237714 ,\n","       -0.7106221 , -1.2419355 , -2.4970138 , -0.18150897,  0.6735854 ,\n","        0.26528636,  0.5322878 ,  0.73154294,  1.1609933 ,  0.35757673],\n","      dtype=float32)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# Look at the vector representation of the word\n","word2vec_g.wv['american']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700086417041,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"4HCuP0bjBCoV","outputId":"5ad69266-bbc0-4910-bd28-215cb704ea9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary: 49732\n","Model Voc: 49732\n"]}],"source":["# Comparing the sizes\n","print(\"Vocabulary:\", len(vocab))\n","print(\"Model Voc:\", len(word2vec_g.wv.key_to_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyNacAmwOfzE"},"outputs":[],"source":["# Now I will create a word to vector dictonary\n","word_vector_word2vec_g = {word: torch.tensor(word2vec_g.wv[word], dtype=torch.float64) for word in word2vec_g.wv.key_to_index}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1700089745691,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"KhXl2SJWOrAR","outputId":"74b629c1-5944-4ab5-eb44-b417efe8f145"},"outputs":[{"data":{"text/plain":["tensor([-0.0017,  0.0517, -0.0601, -0.1193,  0.0810, -0.0496,  0.1552, -0.1221,\n","        -0.1558, -0.1227, -0.0630, -0.2072,  0.0954, -0.1832, -0.0859, -0.0925,\n","        -0.0982,  0.0189,  0.0013, -0.0910, -0.1946,  0.0706,  0.0363, -0.0395,\n","        -0.0470,  0.1158, -0.0036, -0.0930, -0.2082, -0.0980,  0.1134, -0.0618,\n","         0.0077,  0.0110,  0.0144, -0.0236, -0.0692, -0.0868,  0.0715, -0.1377,\n","         0.0241, -0.0865,  0.1088,  0.1833,  0.0884,  0.1197,  0.0771, -0.0391,\n","        -0.0354, -0.1800,  0.0297, -0.0776,  0.0789, -0.1959, -0.1693,  0.2475,\n","         0.0293, -0.0040, -0.1872, -0.0751,  0.0331,  0.0013,  0.0260,  0.0571,\n","         0.0355,  0.0668,  0.0119, -0.0027, -0.0550,  0.0474, -0.1716,  0.0815,\n","         0.0351, -0.0857,  0.0347,  0.1434, -0.0383,  0.0140, -0.0289,  0.1766,\n","        -0.0040, -0.0828, -0.2161,  0.0448,  0.0115,  0.1626,  0.0422, -0.1833,\n","         0.0167,  0.1673, -0.0160, -0.0841, -0.0659, -0.0414, -0.0339, -0.0357,\n","        -0.0113,  0.0326,  0.0860,  0.0713], dtype=torch.float64)"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["# Let's look at a vector\n","word_vector_word2vec_g['american']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yf2ht4HPTi8s"},"outputs":[],"source":["# Save the embeddings\n","with open('./word_vector_word2vec_g.pkl', 'wb') as f:\n","    pickle.dump(word_vector_word2vec_g, f)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"T2EddsrXTi8y"},"outputs":[],"source":["# Load embeddings\n","with open('./word_vector_word2vec_g.pkl', 'rb') as f:\n","    word_vector_word2vec_g = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"V5ydWg249x5V"},"source":["2.- Manuel - Word2vec"]},{"cell_type":"markdown","metadata":{"id":"PVw0u4VP9x5V"},"source":["SKIPGRAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mkf5u3SzLvDU"},"outputs":[],"source":["# Get a training data\n","window_size = 2\n","training = []\n","\n","for sentence in process_corpus:\n","    indices = [vocab[word] for word in sentence]\n","    for i in range(len(indices)):\n","        target = indices[i]\n","        context = indices[max(i - window_size, 0) : i] + \\\n","                  indices[i + 1 : i + window_size + 1]\n","        for ctx_word in context:\n","            training.append((target, ctx_word))\n","\n","# Shuffle the training pairs\n","random.shuffle(training)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1700086450352,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"6-WDmhl9Qxrk","outputId":"54020486-5b89-4f34-ec40-87e6cc36ac14"},"outputs":[{"data":{"text/plain":["3320216"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["# Size of training data\n","len(training)"]},{"cell_type":"markdown","metadata":{"id":"y6eiFPd5EemD"},"source":["Minimise Training Data - *Not used*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9C-dvjVHL650"},"outputs":[],"source":["training_clean = []\n","\n","# Dictionary to keep track of word occurrences\n","word_occurrences = defaultdict(int)\n","\n","for target, ctx_word in training:\n","    if word_occurrences[target] < 500:\n","        training_clean.append((target, ctx_word))\n","        word_occurrences[target] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700086757727,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"ZdGOYw54KISU","outputId":"aca0435a-d470-4db7-81ec-e7a3b658610a"},"outputs":[{"data":{"text/plain":["1902373"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["# Final lenght of the data\n","len(training_clean)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjjTNMTRMmzg"},"outputs":[],"source":["# Create a costum dataset for our project\n","class Test_skipgram(Dataset):\n","    def __init__(self, training):\n","        self.training = training\n","\n","    def __len__(self):\n","        return len(self.training)\n","\n","    def __getitem__(self, idx):\n","        input = torch.tensor(self.training[idx][0])\n","        output = torch.tensor(self.training[idx][1])\n","        return input, output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-H1lcSjXNlaE"},"outputs":[],"source":["# Create a dataset\n","train_dataset = Test_skipgram(training)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeFQ1r1INybG"},"outputs":[],"source":["# Create a batch loader\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"3KykT6jd9x5V"},"outputs":[],"source":["class SkipGramModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim = 100):\n","        super(SkipGramModel, self).__init__()\n","        # Embeding\n","        self.embeddings = nn.Embedding(\n","            num_embeddings = vocab_size,\n","            embedding_dim = embedding_dim)\n","\n","        # Linear\n","        self.linear = nn.Linear(\n","            in_features = embedding_dim,\n","            out_features = vocab_size)\n","\n","    def forward(self, input_word):\n","        embed = self.embeddings(input_word)\n","        out = self.linear(embed)\n","        return out"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":542,"status":"ok","timestamp":1700086884257,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"HZ4YYtosOLUE","outputId":"439dd01a-b709-47c4-96e3-229840781e24"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# configure device, use 'cpu' if cuda is not available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(device)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"elfb8vi9Od1f"},"outputs":[],"source":["# Create the model and move it to the GPU if available\n","wrod2vec_skp = SkipGramModel(vocab_size = len(vocab)).to(device)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700086915611,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"ZxLor-f_OzW5","outputId":"3727a531-2576-43ba-a63c-59ad09468008"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Parameters: 9996132\n","Trainable Parameters: 9996132\n"]}],"source":["# Calculate the total number of parameters\n","total_params = sum(p.numel() for p in wrod2vec_skp.parameters())\n","\n","# Calculate the number of trainable parameters\n","trainable_params = sum(p.numel() for p in wrod2vec_skp.parameters() if p.requires_grad)\n","\n","print(f\"Total Parameters: {total_params}\")\n","print(f\"Trainable Parameters: {trainable_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ji6fHPGPBUl"},"outputs":[],"source":["# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(wrod2vec_skp.parameters(), lr=0.01, weight_decay=5e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1374258,"status":"ok","timestamp":1700088499315,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"S1hlzCs5PIl2","outputId":"6764682d-fa7e-4b0b-8720-7d3935336a98"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training:  10%|█         | 1/10 [02:17<20:35, 137.26s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1/10, Training Loss: 10.8329, Training Accuracy: 0.19%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  20%|██        | 2/10 [04:31<18:04, 135.61s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 2/10, Training Loss: 10.5761, Training Accuracy: 0.76%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  30%|███       | 3/10 [06:47<15:48, 135.48s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 3/10, Training Loss: 10.3837, Training Accuracy: 0.84%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  40%|████      | 4/10 [09:01<13:30, 135.16s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 4/10, Training Loss: 10.2397, Training Accuracy: 0.85%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  50%|█████     | 5/10 [11:18<11:18, 135.71s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 5/10, Training Loss: 10.1321, Training Accuracy: 0.86%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  60%|██████    | 6/10 [13:33<09:02, 135.52s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 6/10, Training Loss: 10.0509, Training Accuracy: 0.86%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  70%|███████   | 7/10 [15:47<06:45, 135.16s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 7/10, Training Loss: 9.9888, Training Accuracy: 0.84%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  80%|████████  | 8/10 [18:11<04:35, 137.85s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 8/10, Training Loss: 9.9405, Training Accuracy: 0.83%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  90%|█████████ | 9/10 [20:39<02:21, 141.03s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 9/10, Training Loss: 9.9025, Training Accuracy: 0.82%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 10/10 [22:54<00:00, 137.50s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 10/10, Training Loss: 9.8722, Training Accuracy: 0.81%\n","\n","Finished Training\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Define number of epochs\n","num_epochs = 10\n","\n","# Training loop\n","for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n","    # Training parametres for each epoch\n","    wrod2vec_skp.train()\n","    running_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    # Loop for each batch of the training\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = wrod2vec_skp(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Calculate training accuracy for the current batch\n","        _, predicted = torch.max(outputs, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","    # Update the learning rate using the scheduler\n","    #scheduler.step()\n","\n","    # Calculate training accuracy for the entire epoch\n","    train_accuracy = 100 * correct_train / total_train\n","\n","    # Calculate learning rate for the epoch\n","    #current_lr = scheduler.get_last_lr()[0]\n","\n","    # # Validation phase\n","    # model.eval()\n","    # val_loss = 0.0\n","    # correct_val = 0\n","    # total_val = 0\n","\n","    # # Disable gradient calculation\n","    # with torch.no_grad():\n","    #     for inputs, labels in val_loader:\n","    #         inputs, labels = inputs.to(device), labels.to(device)\n","    #         outputs = model(inputs)\n","    #         loss = criterion(outputs, labels)\n","    #         val_loss += loss.item()\n","\n","    #         _, predicted = torch.max(outputs, 1)\n","    #         total_val += labels.size(0)\n","    #         correct_val += (predicted == labels).sum().item()\n","\n","    # # Calculate validation statistics\n","    # val_accuracy = 100 * correct_val / total_val\n","\n","    # Save the model every five epochs\n","    if (epoch + 1) % 5 == 0:\n","        model_checkpoint = {\n","            'epoch': epoch + 1,\n","            'model_state_dict': wrod2vec_skp.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': running_loss\n","        }\n","        model_save_path = f'wrod2vec_skp_checkpoint_epoch_{epoch + 1}.pt'\n","        torch.save(model_checkpoint, model_save_path)\n","\n","    # Print training and validation statistics\n","    tqdm.write(f'\\nEpoch {epoch + 1}/{num_epochs}, '\n","               #f'Learning Rate: {current_lr:.4f}\\n'\n","               f'Training Loss: {running_loss/len(train_loader):.4f}, '\n","               f'Training Accuracy: {train_accuracy:.2f}%\\n'\n","              # f'Validation Loss: {val_loss/len(val_loader):.4f}, '\n","              # f'Validation Accuracy: {val_accuracy:.2f}%\\n\\n'\n","               )\n","\n","\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1034,"status":"ok","timestamp":1700091294934,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"7g43hgHGU_xu","outputId":"efaaff91-9b67-4f52-9ae6-29dd13c24d79"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# Load the saved parametres\n","checkpoint = torch.load('./wrod2vec_skp_checkpoint_epoch_10.pt', map_location= device)\n","\n","# Load state dict into the model\n","wrod2vec_skp.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqKCIomuXNbK"},"outputs":[],"source":["def find_closest_words(model, word, vocab, idx_to_word, device, n=10):\n","    if word not in vocab:\n","        return \"Word not in vocabulary.\"\n","\n","    # Get the index of the word and its embedding\n","    word_idx = vocab[word]\n","    word_embedding = model.embeddings(torch.tensor([word_idx], device=device)).detach()\n","\n","    # Move all embeddings to the same device as word_embedding\n","    all_embeddings = model.embeddings.weight.data.to(device)\n","\n","    # Compute cosine similarities\n","    cos_similarities = torch.matmul(all_embeddings, word_embedding.t()).squeeze()\n","\n","    # Move the tensor to CPU before converting to NumPy array\n","    top_n_indices = cos_similarities.cpu().argsort(descending=True).numpy().tolist()\n","    closest_words = [idx_to_word[idx] for idx in top_n_indices if idx != word_idx][:n]\n","\n","    return closest_words\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700088721138,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"hCkSKc19XXwa","outputId":"0b493b90-2ccd-4ad1-ba83-80e26b500f0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Closest words to 'american': ['taekwondo', 'unknowingly', 'hamid', 'kazim', 'guaritore', 'flattering', 'goa', 'superstition', 'strada', 'malia']\n"]}],"source":["# Test on word similarity\n","chosen_word = 'american'\n","closest_words = find_closest_words(wrod2vec_skp, chosen_word, vocab, idx_to_word, device, n=10)\n","print(f\"Closest words to '{chosen_word}': {closest_words}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYFpdRvYQhUg"},"outputs":[],"source":["# Now we create a dictionary for the word and embedding\n","word_vector_wrod2vec_skp = {}\n","\n","for word in vocab:\n","  word_num = vocab[word]\n","  word_vector_wrod2vec_skp[word] = wrod2vec_skp.embeddings(torch.tensor(word_num, device = device))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1700090322014,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"0SHWY7B_RtvU","outputId":"37e4a11f-6e71-44ba-8b39-e06dece21de5"},"outputs":[{"data":{"text/plain":["49732"]},"execution_count":109,"metadata":{},"output_type":"execute_result"}],"source":["# Print size\n","len(word_vector_wrod2vec_skp)"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"4y1g93g2TO0j"},"outputs":[{"ename":"NameError","evalue":"name 'word_vector_wrod2vec_skp' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\jmanu\\OneDrive - University of South Australia\\Machine Learning\\Assessment\\Project\\Project.ipynb Cell 78\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y140sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Save the embeddings\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y140sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./word_vector_wrod2vec_skp.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y140sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(word_vector_wrod2vec_skp, f)\n","\u001b[1;31mNameError\u001b[0m: name 'word_vector_wrod2vec_skp' is not defined"]}],"source":["# Save the embeddings\n","with open('./word_vector_wrod2vec_skp.pkl', 'wb') as f:\n","    pickle.dump(word_vector_wrod2vec_skp, f)"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"_NZRBYnzTO0k"},"outputs":[{"ename":"RuntimeError","evalue":"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\jmanu\\OneDrive - University of South Australia\\Machine Learning\\Assessment\\Project\\Project.ipynb Cell 79\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y141sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m word_vector_wrod2vec_skp \u001b[39m=\u001b[39m {}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y141sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./word_vector_wrod2vec_skp.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y141sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     word_vector_word2vec_skp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(f, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1033\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m'\u001b[39m\u001b[39mreadinto\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m   1028\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1029\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1030\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived object of type \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f)\u001b[39m}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctionality.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1033\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1034\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1035\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid magic number; corrupt file?\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\storage.py:241\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1043\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1041\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1042\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1043\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1045\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1047\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:980\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    976\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    978\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m--> 980\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[0;32m    981\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    982\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    983\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m typed_storage\n\u001b[0;32m    984\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n","\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."]}],"source":["# Load embeddings\n","word_vector_wrod2vec_skp = {}\n","\n","with open('./word_vector_wrod2vec_skp.pkl', 'rb') as f:\n","    word_vector_word2vec_skp = torch.load(f, map_location=torch.device('cpu'))"]},{"cell_type":"markdown","metadata":{"id":"ewqkju5BEtcR"},"source":[" 3. Manuel - CBOW *I won't be using this model*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EX4BhswwEv4s"},"outputs":[],"source":["# Create a training data for the CBOW only taking in cosideration when is 4 context words for a target\n","def create_cbow_training_data(processed_corpus, vocab, window_size=2):\n","    batch_input, batch_output = [], []\n","\n","    for sentence in processed_corpus:\n","        sentence_ids = [vocab[word] for word in sentence if word in vocab]\n","        if len(sentence_ids) < window_size * 2 + 1:\n","            continue\n","\n","        for idx in range(window_size, len(sentence_ids) - window_size):\n","            # Get the context words\n","            context = sentence_ids[idx - window_size:idx] + sentence_ids[idx + 1:idx + window_size + 1]\n","            # Get the target word\n","            target = sentence_ids[idx]\n","            batch_input.append(context)\n","            batch_output.append(target)\n","\n","    return torch.tensor(batch_input, dtype=torch.long), torch.tensor(batch_output, dtype=torch.long)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3014,"status":"ok","timestamp":1700088763286,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"rXQGedaLE13V","outputId":"e73893cb-c5eb-4277-fdbf-6c121a2d9fbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input (Context Words): tensor([[   0,    1,    3,    4],\n","        [   1,    2,    4,    5],\n","        [   2,    3,    5,    6],\n","        ...,\n","        [ 521,  173, 2231, 3495],\n","        [ 173, 4973, 3495, 1501],\n","        [4973, 2231, 1501,   85]])\n","Output (Target Words): tensor([   2,    3,    4,  ..., 4973, 2231, 3495])\n"]}],"source":["# Example of Training data\n","training_CBOW = []\n","\n","cbow_input, cbow_output = create_cbow_training_data(process_corpus, vocab, window_size)\n","\n","print(\"Input (Context Words):\", cbow_input)\n","print(\"Output (Target Words):\", cbow_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T16rwwN-Zzsc"},"outputs":[],"source":["# Create a costum dataset for our project\n","class Test_CBOW(Dataset):\n","    def __init__(self, input, output):\n","        self.input = input\n","        self.output = output\n","\n","    def __len__(self):\n","        return len(self.input)\n","\n","    def __getitem__(self, idx):\n","        input = torch.tensor(self.input[idx])\n","        output = torch.tensor(self.output[idx])\n","        return input, output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wswVvG0aRGF"},"outputs":[],"source":["# Create a dataset\n","train_CBOW = Test_CBOW(cbow_input, cbow_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQZhkeSIa1tt"},"outputs":[],"source":["# Create a batch loader\n","train_loader_CBOW = DataLoader(train_CBOW, batch_size=128, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXUHe5pcbiQX"},"outputs":[],"source":["EMBED_DIMENSION = 100\n","EMBED_MAX_NORM = 1\n","\n","class CBOW_Model(nn.Module):\n","    def __init__(self, vocab_size: int):\n","        super(CBOW_Model, self).__init__()\n","        self.embeddings = nn.Embedding(\n","            num_embeddings=vocab_size,\n","            embedding_dim=EMBED_DIMENSION,\n","            max_norm=EMBED_MAX_NORM,\n","        )\n","        self.linear = nn.Linear(\n","            in_features=EMBED_DIMENSION,\n","            out_features=vocab_size,\n","        )\n","    def forward(self, inputs_):\n","        x = self.embeddings(inputs_)\n","        x = x.mean(axis=1)\n","        x = self.linear(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0oBcDDQbvzY"},"outputs":[],"source":["# Create the model and move it to the GPU if available\n","word2vec_CBOW = CBOW_Model(vocab_size = len(vocab)).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1537,"status":"ok","timestamp":1700088920874,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"UQ7QqvLnb6Ym","outputId":"6d91a2be-1277-4d84-c967-d15e7430cc31"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Parameters: 9996132\n","Trainable Parameters: 9996132\n"]}],"source":["# Calculate the total number of parameters\n","total_params = sum(p.numel() for p in word2vec_CBOW.parameters())\n","\n","# Calculate the number of trainable parameters\n","trainable_params = sum(p.numel() for p in word2vec_CBOW.parameters() if p.requires_grad)\n","\n","print(f\"Total Parameters: {total_params}\")\n","print(f\"Trainable Parameters: {trainable_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ladff1oacJsD"},"outputs":[],"source":["# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(word2vec_CBOW.parameters(), lr=0.01, weight_decay=5e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135511,"status":"ok","timestamp":1700089148724,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"4s_ZUj0GcPsh","outputId":"45962852-c4c3-43f9-f65c-f862d66f819e"},"outputs":[{"name":"stderr","output_type":"stream","text":["\rTraining:   0%|          | 0/3 [00:00<?, ?it/s]<ipython-input-80-d9e5212fda7d>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input = torch.tensor(self.input[idx])\n","<ipython-input-80-d9e5212fda7d>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  output = torch.tensor(self.output[idx])\n","Training:  33%|███▎      | 1/3 [00:42<01:24, 42.19s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1/3, Training Loss: 2.5726, Training Accuracy: 0.40%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  67%|██████▋   | 2/3 [01:28<00:44, 44.68s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 2/3, Training Loss: 2.5617, Training Accuracy: 0.59%\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 3/3 [02:14<00:00, 44.97s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 3/3, Training Loss: 2.5512, Training Accuracy: 0.59%\n","\n","Finished Training\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Define number of epochs\n","num_epochs = 3\n","\n","# Training loop\n","for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n","    # Training parametres for each epoch\n","    word2vec_CBOW.train()\n","    running_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    # Loop for each batch of the training\n","    for inputs, labels in train_loader_CBOW:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = word2vec_CBOW(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Calculate training accuracy for the current batch\n","        _, predicted = torch.max(outputs, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","    # Update the learning rate using the scheduler\n","    #scheduler.step()\n","\n","    # Calculate training accuracy for the entire epoch\n","    train_accuracy = 100 * correct_train / total_train\n","\n","    # Calculate learning rate for the epoch\n","    #current_lr = scheduler.get_last_lr()[0]\n","\n","    # # Validation phase\n","    # model.eval()\n","    # val_loss = 0.0\n","    # correct_val = 0\n","    # total_val = 0\n","\n","    # # Disable gradient calculation\n","    # with torch.no_grad():\n","    #     for inputs, labels in val_loader:\n","    #         inputs, labels = inputs.to(device), labels.to(device)\n","    #         outputs = model(inputs)\n","    #         loss = criterion(outputs, labels)\n","    #         val_loss += loss.item()\n","\n","    #         _, predicted = torch.max(outputs, 1)\n","    #         total_val += labels.size(0)\n","    #         correct_val += (predicted == labels).sum().item()\n","\n","    # # Calculate validation statistics\n","    # val_accuracy = 100 * correct_val / total_val\n","\n","    # Save the model every five epochs\n","    if (epoch + 1) % 3 == 0:\n","        model_checkpoint = {\n","            'epoch': epoch + 1,\n","            'model_state_dict': word2vec_CBOW.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': running_loss\n","        }\n","        model_save_path = f'word2vec_CBOW_checkpoint_epoch_{epoch + 1}.pt'\n","        torch.save(model_checkpoint, model_save_path)\n","\n","    # Print training and validation statistics\n","    tqdm.write(f'\\nEpoch {epoch + 1}/{num_epochs}, '\n","               #f'Learning Rate: {current_lr:.4f}\\n'\n","               f'Training Loss: {running_loss/len(train_loader):.4f}, '\n","               f'Training Accuracy: {train_accuracy:.2f}%\\n'\n","              # f'Validation Loss: {val_loss/len(val_loader):.4f}, '\n","              # f'Validation Accuracy: {val_accuracy:.2f}%\\n\\n'\n","               )\n","\n","\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":494,"status":"ok","timestamp":1700091339752,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"fTkisjlsVqR3","outputId":"92d74df7-f207-4691-9ab0-66890252ba7c"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":122,"metadata":{},"output_type":"execute_result"}],"source":["# Load the saved parametres\n","checkpoint = torch.load('./word2vec_CBOW_checkpoint_epoch_3.pt')\n","\n","# Load state dict into the model\n","word2vec_CBOW.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700091344300,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"Pxd9sKWCNV_9","outputId":"db0126ec-611e-4812-eba9-bd30d697c1dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Closest words to 'american': ['thinking', 'depopulate', 'lithuania', 'memories', 'cardplayer', 'geetas', 'babygirl', 'abhishek', 'circumstance', 'ouija']\n"]}],"source":["# Test on word similarity\n","chosen_word = 'american'\n","closest_words = find_closest_words(word2vec_CBOW, chosen_word, vocab, idx_to_word, device, n=10)\n","print(f\"Closest words to '{chosen_word}': {closest_words}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7YVKObPSAqn"},"outputs":[],"source":["# Now we create a dictionary for the word and embedding\n","word_vector_word2vec_CBOW = {}\n","\n","for word in vocab:\n","  word_num = vocab[word]\n","  word_vector_word2vec_CBOW[word] = word2vec_CBOW.embeddings(torch.tensor(word_num, device = device))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700090637098,"user":{"displayName":"Manuel Alvarez","userId":"07370512438505603529"},"user_tz":-660},"id":"wegZTyNkSGlK","outputId":"775fb196-18c4-42a4-f7a0-84e1189e2010"},"outputs":[{"data":{"text/plain":["49732"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}],"source":["# Print size\n","len(word_vector_word2vec_CBOW)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFRLvJt3SPpd"},"outputs":[],"source":["# Save the embeddings\n","with open('./word_vector_word2vec_CBOW.pkl', 'wb') as f:\n","    pickle.dump(word_vector_word2vec_CBOW, f)"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"8ak2UW_OS8K2"},"outputs":[{"ename":"RuntimeError","evalue":"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\jmanu\\OneDrive - University of South Australia\\Machine Learning\\Assessment\\Project\\Project.ipynb Cell 96\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y164sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load embeddings\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y164sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./word_vector_word2vec_CBOW.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jmanu/OneDrive%20-%20University%20of%20South%20Australia/Machine%20Learning/Assessment/Project/Project.ipynb#Y164sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     word_vector_word2vec_CBOW \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\storage.py:241\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1043\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1041\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1042\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1043\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1045\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1047\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:980\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    976\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    978\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m--> 980\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[0;32m    981\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    982\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    983\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m typed_storage\n\u001b[0;32m    984\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n","File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\torch\\serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n","\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."]}],"source":["# Load embeddings\n","with open('./word_vector_word2vec_CBOW.pkl', 'rb') as f:\n","    word_vector_word2vec_CBOW = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"Cmh9uolsW1g_"},"source":["4 - Pre-Trained word2vec"]},{"cell_type":"markdown","metadata":{"id":"qF_Im7Alw1Em"},"source":["Glove - 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXfzoZpgw2iI","outputId":"7c342bc5-d2bc-4ad4-923a-fea515ea00a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\jmanu\\AppData\\Local\\Temp\\ipykernel_28532\\3832718750.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n","  glove2word2vec(glove_input_file, word2vec_output_file)\n"]},{"data":{"text/plain":["(400000, 100)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","glove_input_file = './glove.6B.100d.txt'\n","word2vec_output_file = './glove.6B.100d.word2vec.txt'\n","\n","glove2word2vec(glove_input_file, word2vec_output_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3scLD8T09BDT"},"outputs":[],"source":["# Load the model with Gensim\n","model_glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apYK10i59BDT"},"outputs":[],"source":["# Initialize a new Word2Vec model\n","word2vec_glove_100 = Word2Vec(vector_size=100, min_count=1)\n","\n","# Add corpus vocabulary\n","word2vec_glove_100.build_vocab(process_corpus)\n","\n","# Build vocabulary from the GloVe model\n","word2vec_glove_100.build_vocab([list(model_glove.key_to_index.keys())], update=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPDknaKk9BDT"},"outputs":[],"source":["# Transfer learning for words in GloVe\n","total_vocab = len(word2vec_glove_100.wv)\n","for word, index in model_glove.key_to_index.items():\n","    if word in word2vec_glove_100.wv.key_to_index:\n","        word2vec_glove_100.wv.vectors[word2vec_glove_100.wv.key_to_index[word]] = model_glove[word]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eI71T7pz9BDT","outputId":"600eab6a-5690-4a8c-9de4-027733b527a3"},"outputs":[{"data":{"text/plain":["(8533040, 8533040)"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["# Retrain model on project corpus\n","word2vec_glove_100.train(process_corpus, total_examples=len(process_corpus), epochs=10, word_count=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaGkS2rp9BDU","outputId":"8ba336d6-8781-4237-c9ba-878e7c435f2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('america', 0.7897357940673828), ('canadian', 0.7711366415023804), ('americans', 0.7692500352859497), ('british', 0.7675219178199768), ('african', 0.7663124799728394), ('native', 0.7617824077606201), ('u.s.', 0.752103865146637), ('australian', 0.7327556610107422), ('nation', 0.7215123176574707), ('u.s', 0.715589165687561)]\n"]}],"source":["# Find words similar to 'american'\n","similar_words = word2vec_glove_100.wv.most_similar('american')\n","print(similar_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yfp2rzsZ9BDU"},"outputs":[],"source":["# Now I will create a word to vector dictonary for project vocabulary\n","word_vector_word2vec_glove_100 = {word: torch.tensor(word2vec_glove_100.wv[word], dtype=torch.float64) for word in vocab}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwtaSvGb9BDU","outputId":"aeb55266-1a7b-487e-d1ae-d305a159a04d"},"outputs":[{"data":{"text/plain":["tensor([ 0.4821,  2.1065,  2.7069,  0.0364,  1.0853, -1.8900, -0.5241, -4.1762,\n","        -0.9483,  0.4180, -0.8020, -1.3401,  0.9504,  1.4267, -1.7116,  1.0932,\n","         2.9092, -0.4862, -1.4452,  0.7147,  2.1533,  1.2674,  2.4156, -0.8985,\n","         1.2497, -0.5533,  0.2356, -3.6768,  1.4475, -0.0148, -0.7305,  1.3082,\n","        -0.0871, -0.8859,  0.2175, -0.8065, -1.3111,  2.8696, -0.0825,  2.1021,\n","        -3.3625, -2.0109, -0.1507,  2.1596,  1.3362, -0.3025, -0.2484, -0.1010,\n","        -0.4143, -3.1531, -0.7171,  0.7964,  0.8004,  1.5582,  0.5978, -6.1966,\n","        -1.1580, -0.8228,  7.1553,  1.3803,  1.1359,  2.3849,  0.2537, -2.0827,\n","         1.3523, -1.2840, -0.4707,  1.5434, -0.3084, -0.3154, -0.3006, -0.1315,\n","        -2.2160,  0.4938,  0.5986,  1.8397,  1.3205, -0.0694, -3.7630, -1.3379,\n","        -0.0533,  0.5773, -0.3365,  1.6439, -3.6063, -1.4586, -1.0522,  0.5624,\n","         0.4609, -1.8788,  0.3404, -1.5877, -0.4120, -0.7619, -4.0528, -0.2314,\n","        -2.3075, -0.4341,  0.6466,  2.6474], dtype=torch.float64)"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["# Let's look at a vector\n","word_vector_word2vec_glove_100['american']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7NMdiDI9BDU"},"outputs":[],"source":["# Save the embeddings\n","with open('./word_vector_word2vec_glove_100.pkl', 'wb') as f:\n","    pickle.dump(word_vector_word2vec_glove_100, f)"]},{"cell_type":"code","execution_count":244,"metadata":{"id":"yZX-6Eqm9BDU"},"outputs":[],"source":["# Load embeddings\n","with open('./word_vector_word2vec_glove_100.pkl', 'rb') as f:\n","    word_vector_word2vec_glove_100 = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"ZiD159IkNpqK"},"source":["# RNN Classification"]},{"cell_type":"markdown","metadata":{"id":"smuTenc3_oMG"},"source":["Create the Different Datasets for the Classification"]},{"cell_type":"code","execution_count":490,"metadata":{},"outputs":[{"data":{"text/plain":["{'drama': 1374,\n"," 'thriller': 1412,\n"," 'documentary': 1367,\n"," 'comedy': 1435,\n"," 'reality-tv': 1450,\n"," 'horror': 1370,\n"," 'action': 1413,\n"," 'adventure': 1446,\n"," 'western': 1259,\n"," 'family': 1413}"]},"execution_count":490,"metadata":{},"output_type":"execute_result"}],"source":["# I will need to ensure that the lenghts of the descriptions are relatively similar \n","totals_len = {}\n","\n","# Initialize dictionary\n","for genre in df_balanced['Genre'].unique():\n","    totals_len[genre] = 0\n","\n","for sentence, genre in zip(process_corpus, df_balanced['Genre']):\n","    if 5 <= len(sentence) <= 100:\n","        totals_len[genre] += 1\n","\n","totals_len"]},{"cell_type":"code","execution_count":496,"metadata":{},"outputs":[],"source":["# I want to only keep movie descriptions between 20 to 60 words\n","# I will need this variables\n","filtered_descriptions = []\n","filtered_genres = []\n","index = []\n","i = 0\n","\n","# Initialize a dictionary to count genres\n","genre_counts = {}\n","\n","# Iterate through each description and its corresponding genre\n","for description, genre in zip(process_corpus, df_balanced['Genre']):\n","    # Check if the length of the description is between ?\n","    if 5 <= len(description) <= 100:\n","        # Update genre count and check if less than minimum per genre\n","        if genre_counts.get(genre, 0) < 1259:\n","            filtered_descriptions.append(description)\n","            filtered_genres.append(genre)\n","            genre_counts[genre] = genre_counts.get(genre, 0) + 1\n","            index.append(i)\n","    i+=1"]},{"cell_type":"code","execution_count":497,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Title</th>\n","      <th>Genre</th>\n","      <th>Description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A Tear for My Enemy/In'gan Kwa Chonjang (1984)</td>\n","      <td>drama</td>\n","      <td>An American Squad of GI's capture one Chinese ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Second Spring (????)</td>\n","      <td>drama</td>\n","      <td>After a series of unusual encounters, it is ob...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hold the Sun (2009)</td>\n","      <td>drama</td>\n","      <td>A film about our ability and inability to conn...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The Young One (1960)</td>\n","      <td>drama</td>\n","      <td>Game warden Miller lives on an isolated island...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beauregard (2009)</td>\n","      <td>drama</td>\n","      <td>1961, in French Savoie. Pierre Hautefort, a te...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12585</th>\n","      <td>Bhatukali (2014)</td>\n","      <td>family</td>\n","      <td>You can choose your friends but not your famil...</td>\n","    </tr>\n","    <tr>\n","      <th>12586</th>\n","      <td>\"Shab e Zindgi\" (2014)</td>\n","      <td>family</td>\n","      <td>Miriam is a middle-class girl who was married ...</td>\n","    </tr>\n","    <tr>\n","      <th>12587</th>\n","      <td>\"Bitworld\" (2010)</td>\n","      <td>family</td>\n","      <td>An incredibly interactive intergalactic televi...</td>\n","    </tr>\n","    <tr>\n","      <th>12588</th>\n","      <td>All at Sea (1970)</td>\n","      <td>family</td>\n","      <td>Douglas is on an \"educational\" cruise and has ...</td>\n","    </tr>\n","    <tr>\n","      <th>12589</th>\n","      <td>Hollywood Now Film Awards (2017)</td>\n","      <td>family</td>\n","      <td>We will host a LIVE EVENT where the Winners wi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12590 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                Title   Genre  \\\n","0      A Tear for My Enemy/In'gan Kwa Chonjang (1984)   drama   \n","1                                Second Spring (????)   drama   \n","2                                 Hold the Sun (2009)   drama   \n","3                                The Young One (1960)   drama   \n","4                                   Beauregard (2009)   drama   \n","...                                               ...     ...   \n","12585                                Bhatukali (2014)  family   \n","12586                          \"Shab e Zindgi\" (2014)  family   \n","12587                               \"Bitworld\" (2010)  family   \n","12588                               All at Sea (1970)  family   \n","12589                Hollywood Now Film Awards (2017)  family   \n","\n","                                             Description  \n","0      An American Squad of GI's capture one Chinese ...  \n","1      After a series of unusual encounters, it is ob...  \n","2      A film about our ability and inability to conn...  \n","3      Game warden Miller lives on an isolated island...  \n","4      1961, in French Savoie. Pierre Hautefort, a te...  \n","...                                                  ...  \n","12585  You can choose your friends but not your famil...  \n","12586  Miriam is a middle-class girl who was married ...  \n","12587  An incredibly interactive intergalactic televi...  \n","12588  Douglas is on an \"educational\" cruise and has ...  \n","12589  We will host a LIVE EVENT where the Winners wi...  \n","\n","[12590 rows x 3 columns]"]},"execution_count":497,"metadata":{},"output_type":"execute_result"}],"source":["# Create a new DF that I will use for later with the values for the classification\n","df_RNN = df_balanced.iloc[index]\n","df_RNN.reset_index(drop=True)"]},{"cell_type":"code","execution_count":498,"metadata":{},"outputs":[{"data":{"text/plain":["{'drama': 1259,\n"," 'thriller': 1259,\n"," 'documentary': 1259,\n"," 'comedy': 1259,\n"," 'reality-tv': 1259,\n"," 'horror': 1259,\n"," 'action': 1259,\n"," 'adventure': 1259,\n"," 'western': 1259,\n"," 'family': 1259}"]},"execution_count":498,"metadata":{},"output_type":"execute_result"}],"source":["# How many movies we now have \n","genre_counts"]},{"cell_type":"code","execution_count":499,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 629\n","Val: 377\n","Test: 253\n"]}],"source":["# Now we can do the splits for the data\n","# Number of classes\n","num_classes = 10\n","\n","# Number of samples per class\n","samples_per_class = 1259\n","\n","# Splits for train, validate, and test\n","train_split = int(0.5 * samples_per_class)\n","val_split = int(0.3 * samples_per_class)\n","test_split = samples_per_class - train_split - val_split\n","\n","# Total Samples per Class\n","print(\"Train:\", train_split)\n","print(\"Val:\", val_split)\n","print(\"Test:\", test_split)"]},{"cell_type":"code","execution_count":500,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 6290\n","Val: 3770\n","Test: 2530\n"]}],"source":["# Create lists to hold indices for each set\n","train_indices, val_indices, test_indices = [], [], []\n","\n","# Set random state\n","rng = np.random.default_rng(seed=42)\n","\n","# Split indices for each class\n","for class_index in range(num_classes):\n","    start_index = class_index * samples_per_class\n","    indices = list(range(start_index, start_index + samples_per_class))\n","    rng.shuffle(indices)\n","    train_indices.extend(indices[:train_split])\n","    val_indices.extend(indices[train_split:train_split + val_split])\n","    test_indices.extend(indices[train_split + val_split:])\n","\n","# Final lengths per class\n","print(\"Train:\", len(train_indices))\n","print(\"Val:\", len(val_indices))\n","print(\"Test:\", len(test_indices))"]},{"cell_type":"code","execution_count":501,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Counter({'drama': 629, 'thriller': 629, 'documentary': 629, 'comedy': 629, 'reality-tv': 629, 'horror': 629, 'action': 629, 'adventure': 629, 'western': 629, 'family': 629})\n","Counter({'drama': 377, 'thriller': 377, 'documentary': 377, 'comedy': 377, 'reality-tv': 377, 'horror': 377, 'action': 377, 'adventure': 377, 'western': 377, 'family': 377})\n","Counter({'drama': 253, 'thriller': 253, 'documentary': 253, 'comedy': 253, 'reality-tv': 253, 'horror': 253, 'action': 253, 'adventure': 253, 'western': 253, 'family': 253})\n"]}],"source":["# Have a look if they are well distributed \n","print(Counter([filtered_genres[index] for index in train_indices]))\n","print(Counter([filtered_genres[index] for index in val_indices]))\n","print(Counter([filtered_genres[index] for index in test_indices]))"]},{"cell_type":"code","execution_count":502,"metadata":{},"outputs":[{"data":{"text/plain":["{'drama': 0,\n"," 'documentary': 1,\n"," 'comedy': 2,\n"," 'adventure': 3,\n"," 'horror': 4,\n"," 'thriller': 5,\n"," 'action': 6,\n"," 'western': 7,\n"," 'reality-tv': 8,\n"," 'family': 9}"]},"execution_count":502,"metadata":{},"output_type":"execute_result"}],"source":["# I'm going to need to change the number of the classes from 0-9\n","genre_to_index = {genre: index for index, genre in enumerate(genres)}\n","index_to_genre = {index: genre for genre, index in genre_to_index.items()}\n","\n","genre_to_index"]},{"cell_type":"code","execution_count":503,"metadata":{"id":"5M4Co9Bp9x5H"},"outputs":[],"source":["# Create a costum dataset for our RNN, It will process the corpus according to the embedding and it will padd the sentence\n","class Classification_Dataset(Dataset):\n","    def __init__(self, corpus, genres, genre_to_index, embeddings, vocab, max_length = 25):\n","        self.corpus = corpus\n","        self.genres = genres\n","        self.genre_to_index = genre_to_index\n","        self.embeddings = embeddings\n","        self.vocab = vocab\n","        self.idx_to_word = idx_to_word\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.corpus)\n","\n","    def __getitem__(self, idx):\n","        # Genre with new label\n","        genre = self.genre_to_index[self.genres[idx]]\n","\n","        # Description with each word as a Vector\n","        sentence = self.corpus[idx]\n","        embedded_sentence = [self.embeddings[words].float() for words in sentence]\n","        #embedded_sentence = [self.embeddings[words] for words in sentence]\n","\n","        # Padding \n","        if len(embedded_sentence) < self.max_length:\n","            embedded_sentence += [torch.zeros(100) for _ in range(self.max_length - len(embedded_sentence))]\n","        elif len(embedded_sentence) > self.max_length:\n","            embedded_sentence = embedded_sentence[:self.max_length]\n","\n","        # Convert to tensor\n","        embedded_tensor = torch.stack(embedded_sentence)\n","\n","        \n","        return embedded_tensor, genre\n"]},{"cell_type":"code","execution_count":504,"metadata":{"id":"sx5zn-QD9x5H"},"outputs":[],"source":["# Add the data into a dataset\n","dataset = Classification_Dataset(filtered_descriptions, filtered_genres, genre_to_index, \n","                                 embeddings = word_vector_word2vec_glove_100, vocab = vocab)"]},{"cell_type":"code","execution_count":505,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(tensor([[ 0.4821,  2.1065,  2.7069,  ..., -0.4341,  0.6466,  2.6474],\n","        [ 0.1053,  0.3194,  0.3410,  ...,  0.2035,  0.5191, -0.0380],\n","        [-0.7778, -1.9939,  0.6586,  ...,  0.0871, -0.0582,  0.6966],\n","        ...,\n","        [ 0.1267,  1.0687,  1.7751,  ...,  3.3020,  0.0348, -1.7682],\n","        [ 1.6057,  0.7916,  3.3139,  ...,  4.3016,  2.5812, -1.3123],\n","        [ 1.0933, -1.1769,  0.9713,  ...,  1.0247,  1.0844, -1.9717]]), 0)\n","torch.Size([25, 100])\n"]}],"source":["# Look at one sample \n","print(dataset[0])\n","print(dataset[0][0].size())\n"]},{"cell_type":"code","execution_count":506,"metadata":{"id":"fLAAgXgh9x5R"},"outputs":[],"source":["# Define the batch size\n","bs = 32\n","\n","# Create Subset datasets for each split\n","train_dataset = Subset(dataset, train_indices)\n","validate_dataset = Subset(dataset, val_indices)\n","test_dataset = Subset(dataset, test_indices)\n","\n","# Create DataLoader objects for each split, with drop_last to ensure all batches have a size of 64 (Small amount of data loss)\n","train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, drop_last=True)\n","val_loader = DataLoader(validate_dataset, batch_size=bs, shuffle=True, drop_last=True)\n","test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":507,"metadata":{},"outputs":[],"source":["class RNNClassifier(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(RNNClassifier, self).__init__()\n","\n","        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=2, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # x = x.float()\n","        output, hidden = self.rnn(x)\n","        hidden = hidden[-1, :, :]\n","        out = self.fc(hidden)\n","        return out"]},{"cell_type":"code","execution_count":508,"metadata":{},"outputs":[],"source":["RNN_class = RNNClassifier(100, 256, 10).to(device)"]},{"cell_type":"code","execution_count":513,"metadata":{},"outputs":[],"source":["# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(RNN_class.parameters(), lr=0.0001, weight_decay=5e-4)"]},{"cell_type":"code","execution_count":514,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training:  10%|█         | 1/10 [00:07<01:08,  7.66s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1/10, Training Loss: 1.4492, Training Accuracy: 49.28%\n","Validation Loss: 1.6377, Validation Accuracy: 42.31%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  20%|██        | 2/10 [00:14<00:59,  7.42s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 2/10, Training Loss: 1.4483, Training Accuracy: 49.38%\n","Validation Loss: 1.6386, Validation Accuracy: 42.12%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  30%|███       | 3/10 [00:22<00:52,  7.51s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 3/10, Training Loss: 1.4497, Training Accuracy: 49.33%\n","Validation Loss: 1.6387, Validation Accuracy: 42.12%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  40%|████      | 4/10 [00:29<00:43,  7.33s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 4/10, Training Loss: 1.4479, Training Accuracy: 49.31%\n","Validation Loss: 1.6372, Validation Accuracy: 42.25%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  50%|█████     | 5/10 [00:36<00:35,  7.17s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 5/10, Training Loss: 1.4467, Training Accuracy: 49.54%\n","Validation Loss: 1.6380, Validation Accuracy: 42.04%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  60%|██████    | 6/10 [00:43<00:28,  7.19s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 6/10, Training Loss: 1.4485, Training Accuracy: 49.36%\n","Validation Loss: 1.6380, Validation Accuracy: 42.23%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  70%|███████   | 7/10 [00:50<00:21,  7.15s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 7/10, Training Loss: 1.4454, Training Accuracy: 49.57%\n","Validation Loss: 1.6368, Validation Accuracy: 42.31%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  80%|████████  | 8/10 [00:57<00:14,  7.02s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 8/10, Training Loss: 1.4468, Training Accuracy: 49.31%\n","Validation Loss: 1.6367, Validation Accuracy: 41.93%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training:  90%|█████████ | 9/10 [01:04<00:06,  6.86s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 9/10, Training Loss: 1.4456, Training Accuracy: 49.47%\n","Validation Loss: 1.6355, Validation Accuracy: 42.09%\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 10/10 [01:11<00:00,  7.12s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 10/10, Training Loss: 1.4445, Training Accuracy: 49.38%\n","Validation Loss: 1.6381, Validation Accuracy: 42.20%\n","\n","\n","Finished Training\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Define number of epochs\n","num_epochs = 10\n","\n","# Training loop\n","for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n","    # Training parametres for each epoch\n","    RNN_class.train()\n","    running_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    # Loop for each batch of the training\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = RNN_class(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Calculate training accuracy for the current batch\n","        _, predicted = torch.max(outputs, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","    # Update the learning rate using the scheduler\n","    #scheduler.step()\n","\n","    # Calculate training accuracy for the entire epoch\n","    train_accuracy = 100 * correct_train / total_train\n","\n","    # Calculate learning rate for the epoch\n","    #current_lr = scheduler.get_last_lr()[0]\n","\n","    # Validation phase\n","    RNN_class.eval()\n","    val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    # Disable gradient calculation\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = RNN_class(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted == labels).sum().item()\n","\n","    # Calculate validation statistics\n","    val_accuracy = 100 * correct_val / total_val\n","\n","    # Save the model every five epochs\n","    # if (epoch + 1) % 5 == 0:\n","    #     model_checkpoint = {\n","    #         'epoch': epoch + 1,\n","    #         'model_state_dict': RNN_class.state_dict(),\n","    #         'optimizer_state_dict': optimizer.state_dict(),\n","    #         'loss': running_loss\n","    #     }\n","    #     model_save_path = f'RNN_gensim_checkpoint_epoch_{epoch + 1}.pt'\n","    #     torch.save(model_checkpoint, model_save_path)\n","\n","    # Print training and validation statistics\n","    tqdm.write(f'\\nEpoch {epoch + 1}/{num_epochs}, '\n","               #f'Learning Rate: {current_lr:.4f}\\n'\n","               f'Training Loss: {running_loss/len(train_loader):.4f}, '\n","               f'Training Accuracy: {train_accuracy:.2f}%\\n'\n","               f'Validation Loss: {val_loss/len(val_loader):.4f}, '\n","               f'Validation Accuracy: {val_accuracy:.2f}%\\n\\n'\n","               )\n","\n","\n","\n","print('Finished Training')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
